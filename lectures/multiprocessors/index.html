<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Multiprocessors - COMP3211 Musings</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="z5206677"><meta name=description content="Why Increasing CPU frequency helps to increase performance"><meta name=keywords content="featherbear,COMP3211,UNSW"><meta name=generator content="Hugo 0.68.3 with theme even"><link rel=canonical href=../../lectures/multiprocessors/><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../manifest.json><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><link href=../../sass/main.min.46877d277c22ebe08dcb937692b8b1d6e40ef958752120243d0f48fdfabcb35a.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><link rel=stylesheet href=../../css/typedjs.shortcode.css><link rel=stylesheet href=../../css/fixDetails.css><link rel=stylesheet href=../../css/fancyBox.css><meta property="og:title" content="Multiprocessors"><meta property="og:description" content="Why Increasing CPU frequency helps to increase performance"><meta property="og:type" content="article"><meta property="og:url" content="/lectures/multiprocessors/"><meta property="article:published_time" content="2021-04-11T08:07:42+00:00"><meta property="article:modified_time" content="2021-11-02T12:26:48+11:00"><meta itemprop=name content="Multiprocessors"><meta itemprop=description content="Why Increasing CPU frequency helps to increase performance"><meta itemprop=datePublished content="2021-04-11T08:07:42+00:00"><meta itemprop=dateModified content="2021-11-02T12:26:48+11:00"><meta itemprop=wordCount content="1074"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Multiprocessors"><meta name=twitter:description content="Why Increasing CPU frequency helps to increase performance"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=../../ class=logo>COMP3211 Musings</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=../../><li class=mobile-menu-item>Home</li></a><a href=https://github.com/featherbear/UNSW-COMP3211><li class=mobile-menu-item>GitHub</li></a><a href=../../categories/><li class=mobile-menu-item>Categories</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=../../ class=logo>COMP3211 Musings</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=../../>Home</a></li><li class=menu-item><a class=menu-item-link href=https://github.com/featherbear/UNSW-COMP3211>GitHub</a></li><li class=menu-item><a class=menu-item-link href=../../categories/>Categories</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Multiprocessors</h1><div class=post-meta><span class=post-time>2021-04-11</span><div class=post-category><a href=../../categories/lectures/>Lectures</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><a href=#why>Why</a><ul><li></li></ul></li><li><a href=#multiprocessors>Multiprocessors</a><ul><li><a href=#processor-classes>Processor Classes</a></li><li><a href=#uma-uniform-memory-access>UMA (Uniform Memory Access)</a></li><li><a href=#numa-non-uniform-memory-access>NUMA (Non-Uniform Memory Access)</a></li><li><a href=#memory-connected-multiprocessors>Memory-connected Multiprocessors</a></li><li><a href=#cache-coherence>Cache Coherence</a></li></ul></li><li><a href=#memory-consistency>Memory Consistency</a><ul><li><a href=#sequential-consistency-sq>Sequential Consistency (SQ)</a></li><li><a href=#programmer-model>Programmer Model</a></li></ul></li></ul></nav></div></div><div class=post-content><h1 id=why>Why</h1><p><img src=../../uploads/snipaste_2021-04-11_18-48-37.png alt></p><p>Increasing CPU frequency helps to increase performance</p><h3 id=power-consumption>Power Consumption</h3><p>Power = Capacitive Load * VoltageÂ² * Frequency</p><p>Reducing load, voltage and frequency can decrease power consumption</p><hr><p><img src=../../uploads/snipaste_2021-04-11_19-25-10.png alt><br>CPU frequency has hit a "frequency wall" / "power wall" - hard to get faster. Instead of increasing the speed of a core, just add more cores!</p><hr><h1 id=multiprocessors>Multiprocessors</h1><p>All processors perform in parallel, achieving high performance.<br>Power consumption scales linearly.</p><h2 id=processor-classes>Processor Classes</h2><ul><li>SISD - Single instruction, single data</li><li>SIMD - Single instruction, multiple data</li><li>MISD - Multiple instruction, single data</li><li>MIMD - Multiple instruction, multiple data</li></ul><p><img src=../../uploads/snipaste_2021-04-11_19-35-16.png alt></p><h2 id=uma-uniform-memory-access>UMA (Uniform Memory Access)</h2><p><img src=../../uploads/snipaste_2021-04-11_19-51-16.png alt></p><p>All processors share a <strong>single bus</strong> to access memory and I/O resources.<br>Allows for simple data sharing, but restricts performance (exclusive access).<br>All memory locations have similar latencies - hence the <strong>uniform</strong> access.<br>Each processor also has a local cache (which will be invalidated when memory updates??? * cache coherence *)</p><h2 id=numa-non-uniform-memory-access>NUMA (Non-Uniform Memory Access)</h2><h2 id=memory-connected-multiprocessors>Memory-connected Multiprocessors</h2><p><img src=../../uploads/snipaste_2021-04-11_19-52-42.png alt></p><p>Each processor has their own local memory, which is accessible to other processors over a network / bus</p><hr><h2 id=cache-coherence>Cache Coherence</h2><p>Shared-memory multiprocessors have cache coherence problems - copies of the same data need to be updated in all locations. This can be performed in different ways</p><ul><li>Software Implementation<ul><li>During compilation, the compiler identifies data items that may cause cache inconsistency, and disables the cache-ability of those items.</li></ul></li><li>Hardware Implementation<ul><li>Snoop-based Protocol (for UMA)<ul><li>Write-update - Update other caches</li><li>Write-invalidate - Mark data on other caches as invalid</li></ul></li><li>Directory-based Protocol (for NUMA)</li></ul></li></ul><h3 id=mesi---write-invalid--snoop>MESI - Write-Invalid | Snoop</h3><p>Uses four states in the cache line</p><ul><li>Modified - Data modified, data in specific cache</li><li>Exclusive - Data same, data only in the current cache</li><li>Shared - Data same, data in other caches</li><li>Invalid - Data invalid</li></ul><h3 id=eg-read-miss>EG: Read Miss</h3><blockquote><p>Processor 1 wants to access data that is only cached in processor 2</p></blockquote><p>Processor 1 checks it cache (miss), then sends a read miss into the bus.<br>Processor 2 changes its state from Exclusive to Shared<br>Data is loaded from memory</p><blockquote><p>Processor 1 wants to access data that is cached by multiple processors</p></blockquote><p>Processor 1 checks it cache (miss), then sends a read miss into the bus.<br>The processors that contain the data change their state from Exclusive to Shared<br>Data is loaded from memory</p><blockquote><p>Processor 1 wants to access data that has been cached AND modified by another processor 2</p></blockquote><p>Processor 1 checks it cache (miss), then sends a read miss into the bus.<br>Processor 2 sends an alert<br>(Processor 1 waits)<br>Processor 2 writes the updated value into the memory<br>Data is loaded from memory</p><h3 id=eg-read-hit>EG: Read Hit</h3><p>No state change, data is just read</p><h3 id=eg-write-miss>EG: Write Miss</h3><p>Send RWITM signal (Read With Intent To Modify)<br>When the line is loaded, it is immediately marked as modified (even before the modification)<br>Write Policy: fetch-on-write</p><blockquote><p>Some other cache has a modified copy</p></blockquote><p>The cache tells the processor that another processor has a copy of the modified data<br>Processor surrenders the bus and waits<br>Other processor access the bus and writes the modified data to main memory<br>The other processor marks the cache line as invalid<br>The processor then tries again</p><blockquote><p>No other cache has a modified copy</p></blockquote><p>Other caches invalidate its own copy<br>As usual</p><h3 id=eg-write-hit>EG: Write Hit</h3><blockquote><p>Shared</p></blockquote><p>Other caches will modify their state to invalid</p><blockquote><p>Exlusive</p></blockquote><p>No control signal needs to be sent along the bus</p><blockquote><p>Modified</p></blockquote><p>No control signal needs to be sent along the bus.<br>Note: Other processors will already have their cache line marked as invalid</p><hr><h3 id=issues-with-snooping>Issues with Snooping</h3><p>Scalability.<br>A single bus and shared memory prevents processors from simultaneously accessing memory. In addition, each processor block needs to be queried for every operation.</p><h3 id=directory-based-protocol-for-numa>Directory Based Protocol (for NUMA)</h3><p>Processors send signals directly to related processors to keep caches up to date.<br>Uses a directory to keep track of the memory block status</p><p>Each processor has its own directory, which contains entries.<br>Each entry contains a dirty bit, and a presence vector (length n, n is the number of processors)<br>Each address has a 'home directory', given by the value of its address</p><p><img src=../../uploads/snipaste_2021-04-12_00-20-56.png alt></p><p><img src=../../uploads/snipaste_2021-04-12_00-24-26.png alt></p><hr><h1 id=memory-consistency>Memory Consistency</h1><p>The order between accesses to different memory locations is very important.<br>Memory consistency models provide rules that applications follow to work together.</p><h2 id=sequential-consistency-sq>Sequential Consistency (SQ)</h2><blockquote><p>The result of any execution is the same as if the operations of all of the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program - Lamport 1979</p></blockquote><ul><li>Overall memory access is serialised</li><li>Each processor follows the same order as specified by its program</li><li>Writes should also be atomic operations</li></ul><hr><p><img src=../../uploads/snipaste_2021-04-24_14-21-40.png alt></p><ul><li>Memory access is serialised</li><li>Uses a bus, so only one memory address is accessed at a time</li><li>Cache coherence is observed</li><li>Writes are atomic</li><li>Access from a single processor complete in program order</li></ul><hr><p><img src=../../uploads/snipaste_2021-04-24_14-24-53.png alt></p><ul><li>If <em>access</em> never hits the L1 cache, then the system behaves.</li><li>If the L1 has a read hit, completion of accesses can be <strong>out of order</strong><ul><li>A write operation may take longer than a subsequent request to read data that is cached.</li></ul></li><li>To maintain sequential consistency, access to the L1 cache needs to be delayed until there are no pending writes in the buffer</li><li>Nullifies the benefits of a write buffer.</li></ul><hr><p><img src=../../uploads/snipaste_2021-04-24_14-33-02.png alt></p><ul><li><p>Access in issued order do not necessarily get completed in order due to the distribution of memory and varied-length paths in the network</p></li><li><p>Writes are inherently non-atomic as new values will be visible to certain processors whilst still been seen as the old version in others</p></li><li><p>To maintain sequential consistency</p><ul><li>Need to know when a write completes<ul><li>For providing atomicity, delay further instructions to the longest operation</li></ul></li><li>Require acknowledgement messages<ul><li>Wait for all invalidation requests to be acknowledged</li></ul></li><li>Ensure write atomicity<ul><li>Delay access to the new value until all acknowledgements are received</li></ul></li><li>Ensure order form a processor<ul><li>Delay access until the previous one completes</li></ul></li></ul><hr><p>Sequential consistency severely restricts hardware and compiler optimisations, and it does not fully guarantee the single execution result.</p></li></ul><p><em>Instead.... do it as software!</em></p><h2 id=programmer-model>Programmer Model</h2><p>Contract between the programmer and the system</p><ul><li>Programmer provides synchronised programs</li><li>System provides sequential consistency at a higher performance</li></ul><p>The programmer explicitly labels synchronised parts of the program - and data accesses are ordered through synchronisation</p><p>Synchronised (parallel) programs can be generated automatically through the compiler, and can also be explicitly created through easily identified synchronisation constructs. The programmer is responsible for guaranteeing the correct order of data access</p></div><footer class=post-footer><nav class=post-nav><a class=prev href=../../lectures/virtual-memory/><i class="iconfont icon-left"></i><span class="prev-text nav-default">Virtual Memory</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=../../lectures/more-hardware-designs-on-parallel-processing/><span class="next-text nav-default">More Hardware Designs on Parallel Processing</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article><script>(function(f,a,t,h,o,m){a[h]=a[h]||function(){(a[h].q=a[h].q||[]).push(arguments)};o=f.createElement('script'),m=f.getElementsByTagName('script')[0];o.async=1;o.src=t;o.id='fathom-script';m.parentNode.insertBefore(o,m)})(document,window,'//ss.featherbear.cc/tracker.js','fathom');fathom('set','siteId','NEQTU');fathom('trackPageview');</script></div></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:z5206677@student.unsw.edu.au class="iconfont icon-email" title=email></a><a href=https://www.linkedin.com/in/andrewjinmengwong/ class="iconfont icon-linkedin" title=linkedin></a><a href=https://github.com/featherbear class="iconfont icon-github" title=github></a><a href=https://www.instagram.com/_andrewjwong/ class="iconfont icon-instagram" title=instagram></a><a href=../../index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span>
<span class=copyright-year>&copy;
2021
<span class=heart><i class="iconfont icon-heart"></i></span><span class=author>Andrew Wong (z5206677)</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=../../js/main.min.d7b7ada643c9c1a983026e177f141f7363b4640d619caf01d8831a6718cd44ea.js></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-107434487-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script src=../../js/typed.js@2.0.9></script><script src=../../js/typedjs.shortcode.js></script></body></html>
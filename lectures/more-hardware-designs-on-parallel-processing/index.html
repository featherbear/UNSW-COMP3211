<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>More Hardware Designs on Parallel Processing - COMP3211 Musings</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="z5206677"><meta name=description content="Improving Performance Parallelism  Widen the basic word length of the machine  8 bit 16 bit 32 bit 64 bit   Vector execution  Execute a single instruction on multiple pieces of data   Parallel-ise instructions  Instruction-level Pipelining Deep Pipeline The depth of the pipeline is increased to achieve higher clock frequencies (more stages)."><meta name=keywords content="featherbear,COMP3211,UNSW"><meta name=generator content="Hugo 0.68.3 with theme even"><link rel=canonical href=../../lectures/more-hardware-designs-on-parallel-processing/><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../manifest.json><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><link href=../../sass/main.min.46877d277c22ebe08dcb937692b8b1d6e40ef958752120243d0f48fdfabcb35a.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><link rel=stylesheet href=../../css/typedjs.shortcode.css><link rel=stylesheet href=../../css/fixDetails.css><link rel=stylesheet href=../../css/fancyBox.css><meta property="og:title" content="More Hardware Designs on Parallel Processing"><meta property="og:description" content="Improving Performance Parallelism  Widen the basic word length of the machine  8 bit 16 bit 32 bit 64 bit   Vector execution  Execute a single instruction on multiple pieces of data   Parallel-ise instructions  Instruction-level Pipelining Deep Pipeline The depth of the pipeline is increased to achieve higher clock frequencies (more stages)."><meta property="og:type" content="article"><meta property="og:url" content="/lectures/more-hardware-designs-on-parallel-processing/"><meta property="article:published_time" content="2021-04-27T15:03:44+00:00"><meta property="article:modified_time" content="2021-11-02T12:26:48+11:00"><meta itemprop=name content="More Hardware Designs on Parallel Processing"><meta itemprop=description content="Improving Performance Parallelism  Widen the basic word length of the machine  8 bit 16 bit 32 bit 64 bit   Vector execution  Execute a single instruction on multiple pieces of data   Parallel-ise instructions  Instruction-level Pipelining Deep Pipeline The depth of the pipeline is increased to achieve higher clock frequencies (more stages)."><meta itemprop=datePublished content="2021-04-27T15:03:44+00:00"><meta itemprop=dateModified content="2021-11-02T12:26:48+11:00"><meta itemprop=wordCount content="724"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="More Hardware Designs on Parallel Processing"><meta name=twitter:description content="Improving Performance Parallelism  Widen the basic word length of the machine  8 bit 16 bit 32 bit 64 bit   Vector execution  Execute a single instruction on multiple pieces of data   Parallel-ise instructions  Instruction-level Pipelining Deep Pipeline The depth of the pipeline is increased to achieve higher clock frequencies (more stages)."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=../../ class=logo>COMP3211 Musings</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=../../><li class=mobile-menu-item>Home</li></a><a href=https://github.com/featherbear/UNSW-COMP3211><li class=mobile-menu-item>GitHub</li></a><a href=../../categories/><li class=mobile-menu-item>Categories</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=../../ class=logo>COMP3211 Musings</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=../../>Home</a></li><li class=menu-item><a class=menu-item-link href=https://github.com/featherbear/UNSW-COMP3211>GitHub</a></li><li class=menu-item><a class=menu-item-link href=../../categories/>Categories</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>More Hardware Designs on Parallel Processing</h1><div class=post-meta><span class=post-time>2021-04-27</span><div class=post-category><a href=../../categories/lectures/>Lectures</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><a href=#improving-performance>Improving Performance</a><ul><li><a href=#parallelism>Parallelism</a></li><li><a href=#instruction-level-pipelining>Instruction-level Pipelining</a></li><li><a href=#thread-level>Thread Level</a></li><li><a href=#system-level---gpus>System Level - GPUs</a></li></ul></li><li><a href=#note-cuda>Note: CUDA</a></li></ul></nav></div></div><div class=post-content><h1 id=improving-performance>Improving Performance</h1><h2 id=parallelism>Parallelism</h2><ul><li>Widen the basic word length of the machine<ul><li>8 bit</li><li>16 bit</li><li>32 bit</li><li>64 bit</li></ul></li><li>Vector execution<ul><li>Execute a single instruction on multiple pieces of data</li></ul></li><li><strong>Parallel-ise instructions</strong></li></ul><h2 id=instruction-level-pipelining>Instruction-level Pipelining</h2><h3 id=deep-pipeline>Deep Pipeline</h3><p>The depth of the pipeline is increased to achieve higher clock frequencies (more stages).</p><p><img src=../../uploads/snipaste_2021-04-28_01-43-25.png alt></p><p>Limitations</p><ul><li>Stage delay cannot be arbitrarily reduced</li><li>There is a delay for each register pipeline required</li><li>Pipeline flush penalty will discard more instruction</li><li>Memory hierarchy can stall executions</li></ul><h3 id=cpu-with-parallel-processing>CPU with Parallel Processing</h3><p>Multiple execution components can be performed simultaneously by having parallel groups of instructions</p><p><img src=../../uploads/snipaste_2021-04-28_01-48-13.png alt></p><h4 id=software-vliw-architecture>(Software) VLIW Architecture</h4><blockquote><p>VLIW - Very Long Instruction Word</p></blockquote><p><img src=../../uploads/snipaste_2021-04-28_01-53-52.png alt></p><ul><li>Issues with more instructions in parallel<ul><li>May create more data hazards</li><li>Forwarding in the pipelined datapath becomes hard</li><li>Identifying parallel instructions is not easy</li></ul></li><li>More aggressive scheduling is required</li></ul><p><img src=../../uploads/snipaste_2021-04-29_00-23-40.png alt><br>Above: The SUB.D instruction does not use the previous <code>F8</code> register value. To increase performance, we could change the register used for SUB.D - which allows the instructions to be run in parallel as they no longer have a data dependency.</p><p><img src=../../uploads/snipaste_2021-04-29_00-23-49.png alt></p><h4 id=hardware-superscalar-architecture>(Hardware) Superscalar Architecture</h4><h5 id=dynamic-scheduling>Dynamic Scheduling</h5><blockquote><p>TL;DR - Each execution unit has its own queue</p></blockquote><p>The hardware issue component in the processor schedules instructions to different parallel execution units</p><ul><li>Track instruction dependencies to allow instruction execution as soon as all operands are available</li><li>Renaming registers to avoid WAR and WAW hazards</li></ul><p><strong>Issue</strong></p><ul><li>Get next instruction from the queue</li><li>Issue the instruction and related available operands from the register file to a matching reservation station entry if available, else stall</li></ul><p><strong>Execute</strong></p><ul><li>Execute ready instructions in the reservation stations</li><li>Monitor the CDB (Common data bus) for the operands of not-ready instructions</li><li>Execution unit idles until a ready instruction is available</li></ul><p><strong>Write Result</strong></p><ul><li>Results from the EU are sent through the CDB to destinations<ul><li>Reservation station</li><li>Memory load buffers</li><li>Register file</li></ul></li><li>The write operations to the destinations should be controlled to avoid data hazards</li></ul><p><img src=../../uploads/snipaste_2021-04-29_00-28-34.png alt></p><p>Special data structures in the register file, reservation stations and memory buffers are used to detect and eliminate hazards</p><p><strong>Reservation Station</strong></p><p><img src=../../uploads/snipaste_2021-04-29_00-55-25.png alt><br><img src=../../uploads/snipaste_2021-04-29_01-09-38.png alt></p><p><img src=../../uploads/snipaste_2021-04-29_17-22-10.png alt><br>The state table holds the a link to the execution unit index that is using the register</p><p><img src=../../uploads/snipaste_2021-04-29_17-31-46.png alt></p><h5 id=dynamic-execution-with-speculation>Dynamic Execution with Speculation</h5><ul><li>Issue, Execute, Write Result, and COMMIT</li></ul><p>The commit step allows instructions to execute out of order, but force them to commit in the correct execution order</p><p><img src=../../uploads/snipaste_2021-04-29_17-38-23.png alt></p><p><img src=../../uploads/snipaste_2021-04-29_17-38-47.png alt></p><h2 id=thread-level>Thread Level</h2><h3 id=multithreaded-processors>Multithreaded Processors</h3><p>When one thread is not available due to an operation delay (i.e. memory access taking a long time), the processor can switch to another thread</p><h4 id=hardware-level-multithreading>Hardware level multithreading</h4><blockquote><p>Thread switching has less overhead than context-switching</p></blockquote><ul><li>Fast switching between threads</li><li>Requires extra resources: replicated registers, PC, etc</li></ul><h5 id=fine-grained-multithreading>Fine-grained Multithreading</h5><blockquote><p>Round-robin approach</p></blockquote><ul><li>Switch threads after each cycle</li><li>If one thread stalls, another is executed</li></ul><h5 id=coarse-grained-multithreading>Coarse-grained Multithreading</h5><ul><li>Only switch threads on long stalls (i.e. L2-cache miss)</li></ul><p><img src=../../uploads/snipaste_2021-04-30_00-43-48.png alt></p><h4 id=simultaneous-multithreading-smt>Simultaneous Multithreading (SMT)</h4><ul><li>A variation of HW multithreading that uses the resources of superscalar architecture</li><li>Exploits both instruction-level parallelism and thread-level parallelism</li></ul><p><img src=../../uploads/snipaste_2021-04-30_00-59-17.png alt></p><p><img src=../../uploads/snipaste_2021-04-30_01-02-07.png alt></p><h2 id=system-level---gpus>System Level - GPUs</h2><p>Graphics Processing Units are processors developed for processing lots of data at once (i.e. all the pixels on a screen)</p><h3 id=typical-tasks>Typical Tasks</h3><ul><li>HSR - Hidden Surface Removal (Remove hidden parts of a 3D object to be shown on a [2D] screen)</li><li>Shading - Making a flat object look more 3D-like</li><li>Texture Mapping - Providing high frequency details, surface texture, colour information</li></ul><p>Many tasks require a huge level of parallelism, however it is common that all tasks are independent (do not rely on each other)</p><h3 id=example-powerful-but-single-threaded>Example: Powerful but single threaded</h3><p><img src=../../uploads/snipaste_2021-04-30_01-10-55.png alt></p><h3 id=example-cheap-but-multiple-processors>Example: Cheap but multiple processors</h3><p><img src=../../uploads/snipaste_2021-04-30_01-13-33.png alt></p><h4 id=brrrr>BRRRR</h4><p><img src=../../uploads/snipaste_2021-04-30_01-14-35.png alt></p><h3 id=simd>SIMD</h3><p>Since multiple processors are performing the same instruction, just on different data fragments, the instructions can be shared (same fetcher/decoder).</p><p>Each execution unit has its own local memory, and they all share a larger memory</p><p><img src=../../uploads/snipaste_2021-04-30_01-16-28.png alt></p><p><img src=../../uploads/snipaste_2021-04-30_01-17-11.png alt></p><p>In the event that there is a stall (i.e. data not available), then the processors can thread-switch to another thread for continued execution</p><p><strong>Remarks</strong></p><ul><li>Use many cheap cores and run them in parallel<ul><li>Easier than improving a single core by <code>n</code> times</li></ul></li><li>Pack cores full of ALUs and share instruction streams across groups of data sets<ul><li>i.e. SIMD vector</li></ul></li><li>Avoid long stalls by interleaving execution of many threads</li></ul><hr><h1 id=note-cuda>Note: CUDA</h1><p>CUDA (Compute Unified Device Architecture) is NVIDIA's platform to use their GPUs for arbitrary operations that require parallel computation.</p><p>It is executed with some C-like programming language, and unifies all forms of GPU parallelism as a CUDA thread.</p><p><img src=../../uploads/snipaste_2021-04-30_01-24-25.png alt></p></div><footer class=post-footer><nav class=post-nav><a class=prev href=../../lectures/multiprocessors/><i class="iconfont icon-left"></i><span class="prev-text nav-default">Multiprocessors</span>
<span class="prev-text nav-mobile">Prev</span></a></nav></footer></article><script>(function(f,a,t,h,o,m){a[h]=a[h]||function(){(a[h].q=a[h].q||[]).push(arguments)};o=f.createElement('script'),m=f.getElementsByTagName('script')[0];o.async=1;o.src=t;o.id='fathom-script';m.parentNode.insertBefore(o,m)})(document,window,'//ss.featherbear.cc/tracker.js','fathom');fathom('set','siteId','NEQTU');fathom('trackPageview');</script></div></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:z5206677@student.unsw.edu.au class="iconfont icon-email" title=email></a><a href=https://www.linkedin.com/in/andrewjinmengwong/ class="iconfont icon-linkedin" title=linkedin></a><a href=https://github.com/featherbear class="iconfont icon-github" title=github></a><a href=https://www.instagram.com/_andrewjwong/ class="iconfont icon-instagram" title=instagram></a><a href=../../index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span>
<span class=copyright-year>&copy;
2021
<span class=heart><i class="iconfont icon-heart"></i></span><span class=author>Andrew Wong (z5206677)</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=../../js/main.min.d7b7ada643c9c1a983026e177f141f7363b4640d619caf01d8831a6718cd44ea.js></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-107434487-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script src=../../js/typed.js@2.0.9></script><script src=../../js/typedjs.shortcode.js></script></body></html>